#labels Featured
#Using naxsi learning mode (starting from v0.44)


== nx_intercept.py ==

<code>
$ python nx_intercept.py  -h
Usage: python nx_intercept [-h,--help] [-p,--port portnumber]
 [-a,--add-monitoring ip:1.2.3.4|md5:af794f5e532d7a4fa59c49845af7947e]
</code>

The learning daemon itself, even if he does a very limited job. 

Basically, it should be pointed by your /RequestDenied location while in learning mode, in order to receive "to-be-blocked" requests.

When doing so, the nx_intercept.py daemon will store naxsi signatures extracted from the HTTP requests into a MySQL database.

It can as well, store the HTTP requests for further analysis if you ask
 him. (see --add-monitoring - still experimental :p)


== nx_extract.py ==

<code>
Usage : python nx_extract [-h,--help] [-p|--port portnumber] 
[-r|--rules /path/to/naxsi_core.rules] [-a|--allow allow user to specify
rules path]
</code>

The daemon you use to obtain whitelists generated by naxsi. Runs by default on port 8081, you will find help here.

== Setup ==

Configure MySQL in sql_id : database, username, host and password.
And you're good to go !

== Example ==

my sites-enabled/foobar :
<code>
server {
 proxy_set_header Proxy-Connection "";
listen       *:80;
access_log  /tmp/nginx_access.log;
error_log  /tmp/nginx_error.log debug;

server_name blog.memze.ro; 

location / {
     include    /etc/nginx/memzero.rules;
     proxy_pass http://88.191.133.106/;
     proxy_set_header Host blog.memze.ro;
   }

 location /RequestDenied {
      proxy_pass http://127.0.0.1:8000;
   }
}
</code>


my /etc/nginx/memzero.rules :
<code>
LearningMode;
SecRulesEnabled;
#SecRulesDisabled;
DeniedUrl "/RequestDenied";

## check rules
CheckRule "$SQL >= 8" BLOCK;
CheckRule "$RFI >= 8" BLOCK;
CheckRule "$TRAVERSAL >= 4" BLOCK;
CheckRule "$EVADE >= 4" BLOCK;
CheckRule "$XSS >= 8" BLOCK;
</code>

Here, I modified my /etc/hosts to point blog.memze.ro to 127.0.0.1.

Take care of the naxsi-ui.conf in the directory. 
<code>
$ cat sql_id 
user=naxsi
pass=trivialpassword
host=127.0.0.1
name=naxsi_sig
</code>

*sql_id is used by both nx_learning and nx_extract*

nx_intercept.py runs on port 8000 by default.
nx_extract.py runs on port 8081 by default.

While you browse on the website, you will probably generate exceptions.
Those exceptions will be catched by nx_intercept.py (as it is being pointed by /RequestDenied location), and put into the MySQL database.

By pointing your browser at 127.0.0.1:8081, you will land onto nx_extract
nx_extract allows you to get whitelists from generated exceptions.

Follow one of the two proposed links to see your generated whitelists.
As you continue browsing, or by adjusting the 


== Usage ==

Point your browser at 127.0.0.1:8081, you will land on a webpage.
As said in this page :
<code>
A GET request on /get_rules will display the generated rules. Non-optimised rules will be displayed in comment.
</code>

You can control some parameters to control whitelist generation :
<code>
page_hit : Minimum number of pages triggering the same event before proposing the optimisation (ie, if there is more than 10 urls that trigger a rule, the rule will be whitelisted on every url).
Default to 10.

rules_hit : Minimum number of rules hitting the same event on the same page before proposing optimisation (ie, if there is more than 10 differents rules triggered on the same url, all rules will be whitelisted on that url)
Default to 10.
</code>

You can as well provide via rules_file argument, a path to an alternative naxsi core rules file. Rules file will allow to display information about exceptions.

Here, as my website has nothing special and only very few user interaction (and because I'm lazy), I will turn page_hit to 3.
This means, if a rules hits 3 different pages, I will turn it off for
the whole site.

<code>
lynx --dump http://127.0.0.1:8081/get_rules?rules_hit=10&page_hit=3
...
########### End Of Rules Before Optimisation ###########
BasicRule wl:1005 "mz:$HEADERS_VAR:cookie";
BasicRule wl:1010 "mz:$HEADERS_VAR:cookie";
BasicRule wl:1011 "mz:$HEADERS_VAR:cookie";
BasicRule wl:1308 "mz:$HEADERS_VAR:cookie";
BasicRule wl:1309 "mz:$HEADERS_VAR:cookie";
BasicRule wl:1315 "mz:$HEADERS_VAR:cookie";
</code>

If you look at the non-optimised part of the page, you will see :
<code>
#BasicRule wl:1011 "mz:$URL:/|$HEADERS_VAR:cookie";
#2 hits on rule 1308 (parenthesis) on url / from 1 different peers
...
</code>
This way, you know what did you decided to authorize.


I can reuse directly those rules into my /etc/nginx/memzero.rules, turn of LearningMode, and reload nginx.


== Learning Mode ? ==

The global idea behind this is that naxsi will generate whitelists from your traffic while in learning mode.
This will allow those to-be-blocked requests not to turn into false positives.

When it seems (and it generally does) possible, naxsi will factorize rules to limit learning mode length.

For example, if you have 10 generated whitelists like this :
<code>
BasicRule wl:1308 "mz:$URL:/page1|$HEADERS_VAR:cookie";
BasicRule wl:1308 "mz:$URL:/page2|$HEADERS_VAR:cookie";
BasicRule wl:1308 "mz:$URL:/page3|$HEADERS_VAR:cookie";
...
</code>

You can safely suppose that this exception will be triggered in all pages, as it's in the cookie. When something like this happens, nx_extract will propose you the following rule instead :

<code>
BasicRule wl:1308 "mz:$HEADERS_VAR:cookie";
</code>

Which means that you will disable rule 1308 on the cookie header, for 'all' pages. 


Learning of 'free' user input forms is very important as well.